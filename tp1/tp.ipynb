{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCJvlnvsKALE"
      },
      "source": [
        "<center><h2>ALTeGraD 2023<br>Lab Session 1: NMT</h2><h3> Neural Machine Translation</h3> 10 / 10 / 2023<br> Dr. G. Shang and H. Abdine</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DB6pvLvlKbtD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIFlSfYTwk8"
      },
      "source": [
        "## Define the Encoder / Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kc8cQTFkKmif"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
        "        # you should return a tensor of shape (seq, batch, feat)\n",
        "        input_sequence = self.embedding.forward(input)\n",
        "        seq, batch, embedding_dim = input_sequence.shape\n",
        "\n",
        "        h0 = torch.zeros((1, batch, self.rnn.hidden_size), device=input.device)\n",
        "        output, _ = self.rnn.forward(input_sequence, h0)\n",
        "        return output\n",
        "\n",
        "def test_size():\n",
        "    encoder = Encoder(50000, 500, 128, 0)\n",
        "    input = torch.randint(0, 50000, (50, 10))\n",
        "    assert(encoder.forward(input).shape == (50, 10, 128))\n",
        "test_size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn9iO9wNT2p7"
      },
      "source": [
        "## Define the Attention layer / Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JwUAUDL4KmoM"
      },
      "outputs": [],
      "source": [
        "class Seq2seqAtt(nn.Module):\n",
        "    '''\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(Seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
        "\n",
        "    def forward(self, target_h, source_hs):\n",
        "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
        "        # fill the gaps #\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "        concat_output = torch.tanh(self.ff_concat.forward(torch.cat((target_h_rep, source_hs), dim=2)))\n",
        "        scores = self.ff_score(concat_output) # should be of shape (seq, batch, 1)\n",
        "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        norm_scores = torch.softmax(scores, 0)\n",
        "\n",
        "        self.stored_attention = norm_scores.detach().cpu() # Store the attention score for question 3\n",
        "\n",
        "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
        "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes\n",
        "        return ct\n",
        "\n",
        "def test_size():\n",
        "    seq2seqAtt = Seq2seqAtt(128, 64, 64)\n",
        "    target_h = torch.rand((1, 10, 64))\n",
        "    source_hs = torch.rand((50, 10, 64))\n",
        "    ct = seq2seqAtt.forward(target_h, source_hs)\n",
        "    assert(ct.shape == (1, 10, 64))\n",
        "test_size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNnGEa5cT9ka"
      },
      "source": [
        "## Define the Decoder layer / Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h7tLaq4PK90q"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''to be used one timestep at a time\n",
        "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input, source_context, h):\n",
        "        # fill the gaps #\n",
        "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
        "        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
        "\n",
        "        input_sequence = self.embedding.forward(input)\n",
        "        _, h = self.rnn.forward(input_sequence, h)\n",
        "        concatenation = self.ff_concat.forward(torch.cat((source_context, h), dim=2))\n",
        "        tilde_h = torch.tanh(concatenation)\n",
        "        prediction = torch.softmax(self.predict.forward(tilde_h), dim=2)\n",
        "\n",
        "        return prediction, h\n",
        "\n",
        "def test_size():\n",
        "    decoder = Decoder(50000, 500, 128, 0)\n",
        "    input = torch.randint(0, 50000, (50, 10))\n",
        "    source_context = torch.rand((1, 10, 128))\n",
        "    h = torch.rand((1, 10, 128))\n",
        "    prediction, h = decoder.forward(input, source_context, h)\n",
        "    assert(prediction.shape == (1, 10, 50000))\n",
        "    assert(h.shape == (1, 10, 128))\n",
        "test_size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUT6D3JETX8H"
      },
      "source": [
        "# Define the full seq2seq model / Task 4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FYX0K3dNK-c9"
      },
      "outputs": [],
      "source": [
        "class seq2seqModel(nn.Module):\n",
        "    '''the full seq2seq model'''\n",
        "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
        "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
        "     'oov_token','sos_token','eos_token','max_size']\n",
        "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t,\n",
        "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
        "                 oov_token, sos_token, eos_token, max_size):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "\n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        print('max source index',self.max_source_idx)\n",
        "        print('source vocab size',len(vocab_s))\n",
        "\n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        print('max target index',self.max_target_idx)\n",
        "        print('target vocab size',len(vocab_t_inv))\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
        "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n",
        "\n",
        "        if self.do_att:\n",
        "            self.att_mech = Seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
        "\n",
        "    def my_pad(self, my_list):\n",
        "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
        "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        return batch_source, batch_target\n",
        "\n",
        "    def forward(self, input, max_size, is_prod):\n",
        "        if is_prod:\n",
        "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        current_batch_size = input.size(1)\n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        source_hs = self.encoder.forward(input)\n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
        "\n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "\n",
        "        while True:\n",
        "            if self.do_att:\n",
        "                source_context = self.att_mech(target_h, source_hs) # (1, batch, feat)\n",
        "            else:\n",
        "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            prediction, target_h = self.decoder.forward(target_input, source_context, target_h)\n",
        "            logits.append(prediction) # (1, batch, vocab)\n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            target_input = torch.argmax(prediction, dim=2) # (1, batch)\n",
        "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
        "            pos += 1\n",
        "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
        "\n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n",
        "\n",
        "        return to_return\n",
        "\n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size,\n",
        "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
        "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
        "                                      collate_fn=self.my_pad)\n",
        "        tdqm_dict_keys = ['loss', 'test loss']\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
        "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)\n",
        "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
        "\n",
        "                        # are we using the model in production\n",
        "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "\n",
        "                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)\n",
        "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
        "                        total_loss += sentence_loss.item()\n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)\n",
        "                        pbar.set_postfix(tdqm_dict)\n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad() # flush gradient attributes\n",
        "                            sentence_loss.backward() # compute gradients\n",
        "                            optimizer.step() # update\n",
        "                            pbar.update(1)\n",
        "\n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1 # reset\n",
        "\n",
        "            if patience_counter > patience:\n",
        "                break\n",
        "\n",
        "    def sourceNl_to_ints(self, source_nl):\n",
        "        '''converts natural language source sentence into source integers'''\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
        "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
        "                       self.oov_token for elt in source_nl_clean_tok]\n",
        "\n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints\n",
        "\n",
        "    def targetInts_to_nl(self, target_ints):\n",
        "        '''converts integer target sentence into target natural language'''\n",
        "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
        "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
        "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
        "\n",
        "    def predict(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
        "        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        return ' '.join(target_nl)\n",
        "\n",
        "    def save(self, path_to_file):\n",
        "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
        "        attrs['state_dict'] = self.state_dict()\n",
        "        torch.save(attrs, path_to_file)\n",
        "\n",
        "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls, path_to_file):\n",
        "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop('state_dict')\n",
        "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B5RprtnBK-ia"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkVw6lVUIT3"
      },
      "source": [
        "## Prepare the Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "datl5SFtJ9Br"
      },
      "outputs": [],
      "source": [
        "### you can download the data from Moodle and upload them to Colab (data.zip and pretrained_moodle.pt)\n",
        "#!wget -c \"https://uc58b5136370605959a0591d349b.dl.dropboxusercontent.com/cd/0/get/CFWn2mJjQdsdDWDldtVpdMcjkv9U-DCRDkPKVehapwI31NzS4iho7LnNgsvjkGCXzJYvQI4lFq-SWIgRlNHajTFKPlNfsm7zHsk6hb3Q0BO8QhkARmRNHhBq6yNbqSRj-7RR2jHy8Tgt7fNdN_sGlR7N79FP31zOckzBEYaVWizEEg/file#\" -O \"data.zip\"\n",
        "#!wget -c \"https://uc5de1b70fa0a20a0c1573f89d75.dl.dropboxusercontent.com/cd/0/get/CFV5KewBNO-O3_eTXY9EszmkCLJOfYxKlyXOtp1N0oHcYYxP8fhd-63BJiA0cXEpsL9vBlpaMjI0OQno4uyuduNkAji5AHTya686hf-dpxwTC--gAeYylgKqn6zFq_ubcUw3EYfID4S4JlIJVqFV7AgPHwTcLsXJT3PcuxGwCE1ljQ/file#\" -O \"pretrained_moodle.pt\"\n",
        "#!unzip data.zip\n",
        "\n",
        "path_to_data = './'\n",
        "path_to_save_models = './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wZCiFl61LPQj"
      },
      "outputs": [],
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.pairs) # total nb of observations\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx] # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
        "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
        "                  elt[1].split()]] for elt in pairs_tmp]\n",
        "    return pairs_tmp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsAk4ILTkEc"
      },
      "source": [
        "## Training / Task 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kSZ-cvSuLQVt"
      },
      "outputs": [],
      "source": [
        "do_att = True # should always be set to True\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if not is_prod:\n",
        "\n",
        "    pairs_train = load_pairs('train')\n",
        "    pairs_test = load_pairs('test')\n",
        "\n",
        "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
        "        vocab_source = json.load(file) # word -> index\n",
        "\n",
        "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
        "        vocab_target = json.load(file) # word -> index\n",
        "\n",
        "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
        "\n",
        "    print('data loaded')\n",
        "\n",
        "    training_set = Dataset(pairs_train)\n",
        "    test_set = Dataset(pairs_test)\n",
        "\n",
        "    print('data prepared')\n",
        "\n",
        "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
        "\n",
        "    model = seq2seqModel(vocab_s=vocab_source,\n",
        "                         source_language='english',\n",
        "                         vocab_t_inv=vocab_target_inv,\n",
        "                         embedding_dim_s=40,\n",
        "                         embedding_dim_t=40,\n",
        "                         hidden_dim_s=30,\n",
        "                         hidden_dim_t=30,\n",
        "                         hidden_dim_att=20,\n",
        "                         do_att=do_att,\n",
        "                         padding_token=0,\n",
        "                         oov_token=1,\n",
        "                         sos_token=2,\n",
        "                         eos_token=3,\n",
        "                         max_size=30) # max size of generated sentence in prediction mode\n",
        "\n",
        "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
        "    model.save(path_to_save_models + 'my_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0rN4RPToom"
      },
      "source": [
        "## Testing / Task 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UCvZmwWoCTUT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/bastienlcn/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VhXbQjP_YrgY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " If I had a red car, I would be happy. -> si j avais eu voiture voiture , je serais serais heureux . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I have a red car. -> j ai une voiture rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer à jeux jeux jeux vidéo . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " This river is full of fish. -> cette rivière est pleine de poisson . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The fridge is full of food. -> le frigo est plein de nourriture . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The cat fell asleep on the mat. -> le chat s est endormi sur le tapis . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " my brother likes pizza. -> mon frère aime la pizza . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I did not mean to hurt you -> je n ai pas voulu intention de blesser blesser blesser blesser blesser blesser . blesser . blesser . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " She is so mean -> elle est tellement méchant méchant . <EOS>\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> aidez moi à chercher une cravate pour aller avec ceci ! ! ! ! ! ! ! ! ! ! ! ! ! ! <EOS>\n",
            "= = = = = \n",
            " I can't help but smoking weed -> je ne peux pas empêcher de de fumer fumer fumer fumer fumer fumer fumer fumer fumer fumer urgence urgence urgence urgence urgence urgence . urgence urgence . urgence urgence .\n",
            "= = = = = \n",
            " The kids were playing hide and seek -> les enfants jouent cache cache cache cache caché caché caché caché caché caché caché caché caché caché caché caché caché caché caché dentifrice perdre caché risques rapide caché risques éveillés\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace -> le chat s est en du du pression peigne peigne cheminée portail portail portail portail portail portail portail portail indépendant oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux oiseaux\n"
          ]
        }
      ],
      "source": [
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
        "\n",
        "    to_test = ['If I had a red car, I would be happy.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you', # translation of mean in context\n",
        "               'She is so mean',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek',\n",
        "               'The cat fell asleep in front of the fireplace']\n",
        "\n",
        "    for elt in to_test:\n",
        "        print('= = = = = \\n','%s -> %s' % (elt, model.predict(elt)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "j ai une maison blanche . . . . . <EOS>\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 880x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Extract the alignments from the Seq2seqAtt model and draws them using matplotlib\n",
        "\n",
        "source = \"I have a white house.\"\n",
        "attentions = []\n",
        "\n",
        "hook = model.att_mech.register_forward_hook(lambda module, input, output : attentions.append(module.stored_attention))\n",
        "# insides of the model.predict function :\n",
        "source_ints = model.sourceNl_to_ints(source)\n",
        "logits = model.forward(source_ints, model.max_size, True) # (seq) -> (<=max_size, vocab)\n",
        "target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
        "target_nl = model.targetInts_to_nl(target_ints.tolist())\n",
        "# ===================================== #\n",
        "hook.remove()\n",
        "\n",
        "print(model.predict(source))\n",
        "\n",
        "source_size = attentions[0].shape[0]\n",
        "target_size = len(attentions)\n",
        "\n",
        "values = np.transpose(np.array([-attentions[i].numpy() for i in range(target_size)])[:, :, 0])\n",
        "\n",
        "plt.matshow(values, cmap='Greys')\n",
        "\n",
        "yticks = list(range(source_size))\n",
        "ytickslabels = word_tokenize(source, model.source_language)\n",
        "xticks = list(range(target_size))\n",
        "xtickslabels = [model.targetInts_to_nl([i])[0] for i in target_ints.tolist()]\n",
        "\n",
        "plt.yticks(yticks, ytickslabels, rotation=0)\n",
        "plt.xticks(xticks, xtickslabels, rotation=60)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "je n ai pas voulu intention de blesser blesser blesser blesser blesser blesser . blesser . blesser . . . . . . . . . . . . .\n",
            "elle est tellement méchant méchant . <EOS>\n",
            "raison après après après après après après après après . . . . . . . . . . . . . . . . . . . . .\n",
            "raison raison ? un une direction une queue queue queue queue queue queue queue queue . vue vue . vue . vue . vue . vue . mensonge . .\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(\"I did not mean to hurt you\"))\n",
        "print(model.predict(\"She is so mean\"))\n",
        "\n",
        "print(model.predict(\"Right after\"))\n",
        "print(model.predict(\"Right is a direction\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
